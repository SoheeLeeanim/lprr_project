# Local Patch Facial Retargeting Paper Reproduction (LPRR)

This repository is a **personal research reproduction project** based on the paper  
**â€œDeep-Learning-Based Facial Retargeting Using Local Patchesâ€**.

The goal is to faithfully reproduce the **core pipeline and ideas of the paper** with a simplified setup:
- single performer
- single target character (MetaHuman)
- reduced data scale
- focus on understanding and implementation, not optimization or extension

---

## Project Overview

The original paper proposes a **local-patch-based facial retargeting pipeline** composed of three main modules:

1. **APEM (Automatic Patch Extraction Module)**  
   Extracts and aligns local facial patches (lip, left eye, right eye).

2. **RM (Reenactment Module)**  
   Patch-wise autoencoder that reenacts source expressions in the target characterâ€™s appearance.

3. **WEM (Weight Estimation Module)**  
   Regresses reenacted patches into **PCA-compressed blendshape weights** for rig control.

This repository follows the same modular structure and training order.

---

## Current Status

- âœ… APEM implemented (MediaPipe FaceMesh-based patch extraction)
- âœ… RM implemented and trained **per patch** (lip / eye_L / eye_R)
- âœ… RM preview video generation (temporal visualization)
- ğŸš§ WEM implementation (in progress)
- ğŸš§ PCA data generation (Blender script, upcoming)

---

## Directory Structure
```
lprr_project/
â”œâ”€ data/
â”‚  â”œâ”€ source/                 # Source actor patch images
â”‚  â”‚  â”œâ”€ patch_lip/
â”‚  â”‚  â”œâ”€ patch_eye_l/
â”‚  â”‚  â””â”€ patch_eye_r/
â”‚  â”‚
â”‚  â”œâ”€ target/                 # Target character (MetaHuman) patch images
â”‚  â”‚  â”œâ”€ patch_lip/
â”‚  â”‚  â”œâ”€ patch_eye_l/
â”‚  â”‚  â””â”€ patch_eye_r/
â”‚  â”‚
â”‚  â””â”€ pca/                    # PCA assets (generated via DCC)
â”‚     â”œâ”€ pca_basis.npy
â”‚     â”œâ”€ pca_mean.npy
â”‚     â””â”€ pca_weight.npy
â”‚
â”œâ”€ code/
â”‚  â”œâ”€ apem/                   # Patch extraction & alignment
â”‚  â”‚  â”œâ”€ extract_patches.py
â”‚  â”‚  â””â”€ __init__.py
â”‚  â”‚
â”‚  â”œâ”€ rm/                     # Reenactment Module
â”‚  â”‚  â”œâ”€ models/
â”‚  â”‚  â”‚  â””â”€ autoencoder.py
â”‚  â”‚  â”œâ”€ datasets/
â”‚  â”‚  â”‚  â””â”€ patch_dataset.py
â”‚  â”‚  â”œâ”€ train_lip.py         # RM training (lip)
â”‚  â”‚  â”œâ”€ train_eye_l.py       # RM training (left eye)
â”‚  â”‚  â”œâ”€ train_eye_r.py       # RM training (right eye)
â”‚  â”‚  â”œâ”€ video_preview.py           # RM video preview generator
â”‚  â”‚  â””â”€ __init__.py
â”‚  â”‚
â”‚  â”œâ”€ wem/                    # Weight Estimation Module (WIP)
â”‚  â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ train.py
â”‚  â”‚  â”œâ”€ infer.py
â”‚  â”‚  â””â”€ __init__.py
â”‚  â”‚
â”‚  â””â”€ utils/                  # Shared utilities
â”‚     â”œâ”€ image.py
â”‚     â”œâ”€ video.py
â”‚     â””â”€ __init__.py
â”‚
â”œâ”€ checkpoints/               # Trained model checkpoints
â”‚  â”œâ”€ rm_lip/
â”‚  â”œâ”€ rm_eye_l/
â”‚  â””â”€ rm_eye_r/
â”‚
â”œâ”€ samples/                   # Training sample outputs (4-row grids)
â”‚  â”œâ”€ rm_lip/
â”‚  â”œâ”€ rm_eye_l/
â”‚  â””â”€ rm_eye_r/
â”‚
â”œâ”€ runs/                      # Experiment logs (optional)
â””â”€ README.md
```


---

## RM (Reenactment Module) Details

### Training Strategy
- RM is trained **independently per local patch**:
  - lip
  - left eye
  - right eye
- All patches share the **same autoencoder architecture**
- Training is repeated with different datasets, checkpoints, and outputs

### Loss Function
- L1 loss
- SSIM loss  
(as described in the paper)

### Output Visualization
Each RM training script saves 4-row sample images: 
<br>
[source]
[source reconstruction]
[target]
[reenacted target]

<p align="center">
  <img src="samples/rm_eye_l/epoch_008_eye_l_rand.png" width="45%">
  <img src="samples/rm_lip/epoch_004_lip_rand.png" width="45%">
</p>



These samples are also assembled into **temporal preview videos** for debugging stability and alignment.

---

## RM Preview Video

Sample preview videos are included in this repository.

#### â–¶ click image below to Download / Watch RM Preview
[![RM Preview](samples/rm_preview_thumb.png)](samples/rm_preview_lip_eyeR_eyeL.mp4)


Each frame shows:
- lip / left eye / right eye patches side by side
- per patch:
source â†’ reconstruction â†’ target â†’ reenacted target

The preview is used to:
- validate temporal consistency
- detect alignment drift
- verify patch-specific RM behavior

---

## WEM (Weight Estimation Module)

WEM maps reenacted patches to **PCA-compressed blendshape weights**.

Important notes:
- WEM is implemented entirely in Python (PyTorch)
- DCC (Blender) is only required **once** to generate PCA data
- No DCC is used during WEM training or inference

(Currently under development.)

---

## Notes

- This project prioritizes **paper-faithful reproduction**, not performance tuning.
- Local patch size is fixed to **128Ã—128**, following the paper.
- Latent vectors are **not directly used** for regression (paper ablation result).
- Head pose retargeting is intentionally excluded.

---

## Reference

**Deep-Learning-Based Facial Retargeting Using Local Patches**  
https://doi.org/10.1111/cgf.15263

---

## Disclaimer

This is a non-commercial, educational reproduction project.  
All assets are used solely for research and demonstration purposes.


